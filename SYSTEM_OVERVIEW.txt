================================================================================
                    RESUME RAG SYSTEM - COMPLETE OVERVIEW
================================================================================

PROJECT STRUCTURE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  â”Œâ”€ Frontend (React) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ â”œâ”€ rag-ui/src/App.jsx              [Main component]       â”‚
  â”‚ â”œâ”€ rag-ui/src/components/          [Chat interface]       â”‚
  â”‚ â”œâ”€ rag-ui/src/styles/              [CSS styling]          â”‚
  â”‚ â””â”€ rag-ui/public/index.html         [HTML root]           â”‚
  â”‚                                                             â”‚
  â”‚ npm dependencies: react, react-dom, axios, vite           â”‚
  â”‚ Runs on: localhost:3000                                   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â†• axios HTTP
  â”Œâ”€ Backend (Flask) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ â”œâ”€ backend.py                      [REST API]             â”‚
  â”‚ â”‚  â”œâ”€ GET  /health                                        â”‚
  â”‚ â”‚  â”œâ”€ POST /api/query                                     â”‚
  â”‚ â”‚  â”œâ”€ POST /api/load-pdf                                  â”‚
  â”‚ â”‚  â””â”€ GET  /api/stats                                     â”‚
  â”‚ â”‚                                                          â”‚
  â”‚ â”‚ pip: flask, flask-cors                                  â”‚
  â”‚ â””â”€ Runs on: localhost:5000                                â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†• Python import
  â”Œâ”€ RAG Engine (pdf_assessment.py) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                                             â”‚
  â”‚ Pipeline:                                                   â”‚
  â”‚ 1. Extract PDF â†’ PyPDF2                                   â”‚
  â”‚ 2. Chunk text â†’ 500 char chunks                           â”‚
  â”‚ 3. Embed chunks â†’ all-MiniLM-L6-v2                        â”‚
  â”‚ 4. Store in DB â†’ Chromadb                                 â”‚
  â”‚ 5. Retrieve â†’ Top-3 semantic search                       â”‚
  â”‚ 6. Generate â†’ Send to Ollama                              â”‚
  â”‚ 7. Return â†’ Answer + chunks + score                       â”‚
  â”‚                                                             â”‚
  â”‚ pip: PyPDF2, chromadb, requests, ollama                   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†• HTTP REST                    â†• HTTP REST
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Chromadb Vector DB   â”‚      â”‚  Ollama LLM API        â”‚
  â”‚                       â”‚      â”‚                        â”‚
  â”‚ â€¢ All embeddings      â”‚      â”‚ â€¢ Model: gemma:2b      â”‚
  â”‚ â€¢ Vector storage      â”‚      â”‚ â€¢ Text generation      â”‚
  â”‚ â€¢ Similarity search   â”‚      â”‚ â€¢ No internet needed   â”‚
  â”‚ â€¢ Cosine distance     â”‚      â”‚ â€¢ Local inference      â”‚
  â”‚                       â”‚      â”‚                        â”‚
  â”‚ localhost (embedded)  â”‚      â”‚ localhost:11434        â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


DATA FLOW - USER QUERY:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

User Types: "What is the candidate's current role?"
         â†“
React Frontend captures input
         â†“
axios POST to /api/query
    { question: "What is the candidate's current role?" }
         â†“
Flask Backend receives request
         â†“
RAGApp.query() executes:
    â”œâ”€ retrieve(question)
    â”‚  â”œâ”€ Convert question to embedding (all-MiniLM-L6-v2)
    â”‚  â”œâ”€ Search Chromadb vector store
    â”‚  â”œâ”€ Get top-3 similar chunks
    â”‚  â””â”€ Return: [chunk1, chunk2, chunk3] with scores
    â”‚
    â”œâ”€ generate_response(question, chunks)
    â”‚  â”œâ”€ Combine question + context
    â”‚  â”œâ”€ Send to Ollama API
    â”‚  â”œâ”€ gemma:2b generates answer
    â”‚  â””â”€ Stream or get complete response
    â”‚
    â””â”€ Format response
         â”œâ”€ Answer text
         â”œâ”€ Retrieved chunks with scores
         â””â”€ Processing time
         â†“
Flask returns JSON:
{
  "response": "The candidate is a Senior Consultant...",
  "chunks": [
    { "text": "Senior Consultant at Capgemini...", "score": 0.945 },
    { "text": "Professional Experience section...", "score": 0.873 },
    { "text": "July 2022 â€“ January 2026...", "score": 0.821 }
  ],
  "retrievalTime": 2.34
}
         â†“
React receives response
         â†“
Display in chat interface:
â”œâ”€ User message bubble
â”œâ”€ Assistant response
â”œâ”€ Expandable chunks section
â””â”€ Response timing


PORTS & ENDPOINTS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Frontend:
  http://localhost:3000

Backend APIs:
  GET  http://localhost:5000/health          â† Server status
  POST http://localhost:5000/api/query       â† Ask question
  POST http://localhost:5000/api/load-pdf    â† Load PDF
  GET  http://localhost:5000/api/stats       â† Statistics

Ollama:
  http://localhost:11434                     â† LLM API
  Model: gemma:2b


FILES & PURPOSES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

CORE APPLICATION:
  backend.py                 Flask REST API server
  pdf_assessment.py          RAG engine (existing, integrated)
  rag-ui/                    React frontend

CONFIGURATION:
  rag-ui/vite.config.js      Vite build configuration
  rag-ui/package.json        npm dependencies & scripts

DOCUMENTATION:
  QUICKSTART.md              5-minute getting started
  README_FINAL.md            Complete reference
  SETUP_COMPLETE.md          Detailed setup info
  rag-ui/README.md           React/API documentation
  SYSTEM_OVERVIEW.txt        This file
  requirements.txt           Python dependencies

UTILITIES:
  start.sh                   One-click start all services
  verify-setup.sh            Verify installation


QUICK START:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Prerequisite: Make sure Ollama is running
  $ ollama serve

Then run:
  $ cd /workspaces/dotnetcrudservice
  $ bash start.sh

Or manually:
  Terminal 1: python3 backend.py
  Terminal 2: cd rag-ui && npm run dev

Then open: http://localhost:3000


PERFORMANCE METRICS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

First Query:           3-5s  (model warm-up)
Subsequent Queries:    1-3s  (model cached)
Vector Search:       0.5-1s  (Chromadb)
LLM Generation:      1-3s    (Ollama gemma:2b)
Frontend Load:       <1s     (React dev)


SUGGESTED QUESTIONS (Pre-configured):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. "What is the candidate's current role?"
2. "What technologies does the candidate know?"
3. "How many years of experience does the candidate have?"
4. "What are the main projects listed in the resume?"
5. "What companies has the candidate worked for?"
6. "What are the candidate's key skills and expertise?"


TROUBLESHOOTING QUICK REFERENCE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Problem                     Solution
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Backend not responding      python3 backend.py
React shows blank page      Hard refresh (Ctrl+Shift+R)
No Ollama connection        ollama serve
Port already in use         lsof -i :PORT â†’ kill PID
Slow responses              Wait for warm-up, check resources
Error in browser console    Check F12 DevTools

Verify setup:
  $ bash verify-setup.sh


ARCHITECTURE SUMMARY:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

LAYER 1: Frontend UI
  â””â”€ React 19, Vite, Axios
     â””â”€ Chat interface with 6 suggested questions

LAYER 2: API Gateway
  â””â”€ Flask with CORS
     â””â”€ REST endpoints with error handling

LAYER 3: RAG Engine
  â””â”€ PDF â†’ Chunks â†’ Embeddings â†’ Vector DB â†’ Semantic Search
     â””â”€ + LLM Generation â†’ Formatted Response

LAYER 4: Inference
  â””â”€ Chromadb (vector search)
     â””â”€ Ollama API (LLM inference)


NEXT STEPS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Verify setup:          bash verify-setup.sh
2. Start Ollama:          ollama serve
3. Start application:     bash start.sh
4. Open browser:          http://localhost:3000
5. Ask questions!         Click suggested questions or type


================================================================================
                              READY TO USE! ğŸš€
================================================================================
